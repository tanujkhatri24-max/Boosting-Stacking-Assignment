{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPzyVfxH4RTUeAH+CRnkpdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tanujkhatri24-max/Boosting-Stacking-Assignment/blob/main/Boosting_%26_Stacking.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Boosting & Stacking**\n",
        "#Assignment\n"
      ],
      "metadata": {
        "id": "M0y7MaywuKvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. What is Boosting in Machine Learning? Explain how it improves weak learners?\n",
        "\n",
        "**Boosting** is an **ensemble learning technique** in machine learning that combines multiple **weak learners** to form a **strong predictive model**.\n",
        "\n",
        "A **weak learner** is a model that performs only slightly better than random guessing. Boosting improves performance by **training models sequentially**, where each new model focuses on correcting the **errors made by previous models**.\n",
        "\n",
        "### **How Boosting Improves Weak Learners**\n",
        "\n",
        "1. **Sequential Learning**\n",
        "   Models are trained one after another. Each new model learns from the **mistakes of the previous model**, rather than learning independently.\n",
        "\n",
        "2. **Error-Focused Training**\n",
        "   Data points that are **misclassified** by earlier models are given **higher importance (weights)** so that subsequent models focus more on difficult cases.\n",
        "\n",
        "3. **Weighted Contribution**\n",
        "   Each weak learner is assigned a **weight based on its performance**. More accurate learners have a greater influence on the final prediction.\n",
        "\n",
        "4. **Combination of Predictions**\n",
        "   The final prediction is made by **combining all weak learners’ outputs**, usually through weighted voting or weighted averaging.\n",
        "\n",
        "\n",
        "#2.  What is the difference between AdaBoost and Gradient Boosting in terms of how models are trained?\n",
        "   -  **AdaBoost** and **Gradient Boosting** are both boosting algorithms, but they differ in **how models are trained and how errors are corrected**.\n",
        "\n",
        "-  **AdaBoost (Adaptive Boosting)** trains models **sequentially** by adjusting the **weights of training samples**. Initially, all data points are given equal importance. After each model is trained, the weights of **misclassified samples are increased**, and correctly classified samples are given lower weights. The next model focuses more on the difficult samples. Each weak learner is also assigned a **weight based on its accuracy**, and final predictions are made using **weighted voting**. AdaBoost is highly sensitive to **noisy data and outliers** because misclassified points receive increasing attention.\n",
        "\n",
        "- **Gradient Boosting**, on the other hand, trains models sequentially by **optimizing a loss function using gradient descent**. Instead of reweighting data points, each new model is trained to **predict the residual errors (gradients)** of the previous model.\n",
        "- The model gradually improves by minimizing the overall loss function step by step. Gradient Boosting is more **flexible** because it supports different loss functions and includes **regularization techniques**, making it more robust to noise.\n",
        "\n",
        "#3. How does regularization help in XGBoost?\n",
        "   -   **XGBoost** (Extreme Gradient Boosting) is a powerful boosting algorithm that uses **regularization** to control model complexity and prevent **overfitting**, which is common in highly flexible models.\n",
        "\n",
        "### **Role of Regularization in XGBoost**\n",
        "\n",
        "1. **Tree Complexity Control**\n",
        "   XGBoost adds a regularization term to its objective function that **penalizes complex trees**. This discourages the model from growing unnecessarily deep trees.\n",
        "\n",
        "2. **L1 and L2 Regularization**\n",
        "\n",
        "   * **L1 regularization (α)** encourages sparsity by reducing less important feature weights to zero.\n",
        "   * **L2 regularization (λ)** smooths leaf weights, preventing extreme predictions.\n",
        "\n",
        "3. **Minimum Loss Reduction (γ)**\n",
        "   A split is made only if it results in a **minimum reduction in loss (gamma)**. This avoids creating splits that do not significantly improve performance.\n",
        "\n",
        "4. **Limiting Tree Growth**\n",
        "   Parameters like **max_depth** and **min_child_weight** restrict how large and complex trees can become.\n",
        "\n",
        "5. **Shrinkage (Learning Rate)**\n",
        "   The learning rate reduces the contribution of each tree, forcing the model to learn **gradually**, which improves generalization.\n",
        "\n",
        "#4. Why is CatBoost considered efficient for handling categorical data?\n",
        "   -  **CatBoost** is a gradient boosting algorithm specifically designed to handle **categorical features efficiently** without extensive preprocessing.\n",
        "\n",
        "### **Reasons for Its Efficiency**\n",
        "\n",
        "1. **No Need for One-Hot Encoding**\n",
        "\n",
        "   * Unlike traditional algorithms, CatBoost can work directly with categorical features.\n",
        "   * This avoids creating a large number of dummy variables, which **reduces memory usage and improves speed**.\n",
        "\n",
        "2. **Ordered Target Encoding**\n",
        "\n",
        "   * CatBoost uses **target-based statistics** (like mean target values) to convert categorical features into numerical form.\n",
        "   * It applies **ordered boosting**, calculating statistics in a way that **prevents target leakage**, which improves generalization.\n",
        "\n",
        "3. **Handles High-Cardinality Features**\n",
        "\n",
        "   * Categorical features with many unique values (like city names or product IDs) are processed efficiently using **special encoding schemes**.\n",
        "   * This avoids overfitting and preserves meaningful patterns.\n",
        "\n",
        "4. **Built-in Support for Categorical Features in Trees**\n",
        "\n",
        "   * Decision trees in CatBoost can **split directly on categorical values** using optimized algorithms.\n",
        "   * This reduces preprocessing steps and speeds up training.\n",
        "\n",
        "5. **Better Accuracy and Stability**\n",
        "\n",
        "   * By handling categorical data natively, CatBoost achieves **higher accuracy** and is **less prone to overfitting** compared to other boosting algorithms that rely on one-hot encoding.\n",
        "\n",
        "#5. What are some real-world applications where boosting techniques are preferred over bagging methods?   \n",
        "  -  Boosting is preferred over bagging in scenarios where **accuracy is critical**, **data has complex patterns**, or the goal is to **reduce bias**. Boosting focuses on **correcting errors of previous models**, making it ideal for challenging prediction tasks.\n",
        "\n",
        "### **1. Finance & Banking**\n",
        "\n",
        "* **Credit Scoring & Loan Default Prediction** – Boosting algorithms like XGBoost or CatBoost are used to identify high-risk borrowers by focusing on difficult-to-classify cases.\n",
        "* **Fraud Detection** – Boosting models excel in detecting **rare fraudulent transactions** by giving more weight to misclassified cases.\n",
        "\n",
        "### **2. Healthcare**\n",
        "\n",
        "* **Disease Diagnosis & Prognosis** – Predicting conditions like cancer or heart disease using boosting improves accuracy compared to single models or bagging, especially when patient data is imbalanced.\n",
        "* **Medical Imaging Analysis** – Boosting helps identify subtle patterns in scans by focusing on misclassified samples.\n",
        "\n",
        "### **3. Marketing & Customer Analytics**\n",
        "\n",
        "* **Customer Churn Prediction** – Boosting detects customers likely to leave by learning from misclassified cases in subscription or telecom datasets.\n",
        "* **Recommendation Systems** – Boosting improves ranking predictions by iteratively correcting mistakes.\n",
        "\n",
        "### **4. Insurance**\n",
        "\n",
        "* **Claim Prediction & Risk Assessment** – Boosting models can better predict high-risk claims, reducing financial losses.\n",
        "\n",
        "### **5. E-commerce**\n",
        "\n",
        "* **Sales Forecasting** – Boosting captures complex, non-linear patterns in product demand.\n",
        "* **Click-Through Rate (CTR) Prediction** – Boosting algorithms like LightGBM/XGBoost are widely used in advertising for highly accurate predictions.\n",
        "\n",
        "\n",
        "#6. : Write a Python program to:\n",
        "● Train an AdaBoost Classifier on the Breast Cancer dataset\n",
        "● Print the model accuracy\n",
        "\n",
        " ```python\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize a weak learner (Decision Tree)\n",
        "base_estimator = DecisionTreeClassifier(max_depth=1, random_state=42)\n",
        "\n",
        "# Train the AdaBoost Classifier\n",
        "adaboost = AdaBoostClassifier(\n",
        "    base_estimator=base_estimator,\n",
        "    n_estimators=100,\n",
        "    learning_rate=1.0,\n",
        "    random_state=42\n",
        ")\n",
        "adaboost.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = adaboost.predict(X_test)\n",
        "\n",
        "# Calculate and print accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"AdaBoost Classifier Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```text\n",
        "AdaBoost Classifier Accuracy: 0.953\n",
        "```\n",
        "\n",
        "#7. Write a Python program to:\n",
        "● Train a Gradient Boosting Regressor on the California Housing dataset\n",
        "● Evaluate performance using R-squared score\n",
        "\n",
        " ```python\n",
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train Gradient Boosting Regressor\n",
        "gbr = GradientBoostingRegressor(\n",
        "    n_estimators=100,\n",
        "    learning_rate=0.1,\n",
        "    max_depth=3,\n",
        "    random_state=42\n",
        ")\n",
        "gbr.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = gbr.predict(X_test)\n",
        "\n",
        "# Evaluate performance using R-squared score\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "print(\"Gradient Boosting Regressor R-squared Score:\", r2)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```text\n",
        "Gradient Boosting Regressor R-squared Score: 0.825\n",
        "```\n",
        "\n",
        "#8. : Write a Python program to:\n",
        "● Train an XGBoost Classifier on the Breast Cancer dataset\n",
        "● Tune the learning rate using GridSearchCV\n",
        "● Print the best parameters and accuracy\n",
        "\n",
        " ```python\n",
        "# Import required libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize XGBoost Classifier\n",
        "xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
        "\n",
        "# Define hyperparameter grid for learning rate\n",
        "param_grid = {\n",
        "    'learning_rate': [0.01, 0.05, 0.1, 0.2]\n",
        "}\n",
        "\n",
        "# Apply GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=xgb,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Test Set Accuracy:\", accuracy)\n",
        "```\n",
        "\n",
        "**Sample Output:**\n",
        "\n",
        "```text\n",
        "Best Parameters: {'learning_rate': 0.1}\n",
        "Test Set Accuracy: 0.9649\n",
        "```\n",
        "\n",
        "#9. : Write a Python program to:\n",
        "● Train a CatBoost Classifier\n",
        "● Plot the confusion matrix using seaborn\n",
        "\n",
        "```python\n",
        "# Import required libraries\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Load the Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Initialize and train CatBoost Classifier\n",
        "catboost_model = CatBoostClassifier(\n",
        "    iterations=200,\n",
        "    learning_rate=0.1,\n",
        "    depth=5,\n",
        "    verbose=0,\n",
        "    random_state=42\n",
        ")\n",
        "catboost_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = catboost_model.predict(X_test)\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Plot confusion matrix using seaborn\n",
        "plt.figure(figsize=(6,5))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['Benign', 'Malignant'], yticklabels=['Benign', 'Malignant'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('CatBoost Confusion Matrix')\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "\n",
        "#10. You're working for a FinTech company trying to predict loan default using customer demographics and transaction behavior.\n",
        "\n",
        "\n",
        "## **Step 1: Data Preprocessing & Handling Missing/Categorical Values**\n",
        "\n",
        "1. **Handle Missing Values**\n",
        "\n",
        "   * For numeric features: impute using **median** (robust to outliers) or **mean**.\n",
        "   * For categorical features: impute using **mode** or create a **new category \"Unknown\"**.\n",
        "\n",
        "2. **Encode Categorical Features**\n",
        "\n",
        "   * If using **XGBoost or AdaBoost**: use **One-Hot Encoding** or **Target Encoding** for high-cardinality features.\n",
        "   * If using **CatBoost**: pass categorical features directly—CatBoost handles them natively.\n",
        "\n",
        "3. **Feature Scaling (Optional)**\n",
        "\n",
        "   * Boosting algorithms do **not require feature scaling**, but it may help interpret feature importance.\n",
        "\n",
        "4. **Handle Imbalanced Data**\n",
        "\n",
        "   * Use **SMOTE**, **Random Oversampling**, or **class weights** to balance the minority class (loan defaulters).\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 2: Choice Between AdaBoost, XGBoost, or CatBoost**\n",
        "\n",
        "* **AdaBoost**: Simple and works for small datasets; sensitive to noisy data.\n",
        "* **XGBoost**: Highly efficient, flexible, supports regularization, works well with numerical data.\n",
        "* **CatBoost**: Best if the dataset has **many categorical variables**, handles them natively, reduces preprocessing, and avoids target leakage.\n",
        "\n",
        "**Decision:** For this dataset with **imbalanced and categorical features**, **CatBoost** is preferred.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 3: Hyperparameter Tuning Strategy**\n",
        "\n",
        "* Use **GridSearchCV** or **RandomizedSearchCV** with **cross-validation**.\n",
        "\n",
        "* Key hyperparameters for CatBoost/XGBoost:\n",
        "\n",
        "  * `learning_rate` → smaller values reduce overfitting\n",
        "  * `iterations/n_estimators` → number of boosting rounds\n",
        "  * `depth` → controls tree complexity\n",
        "  * `l2_leaf_reg` (CatBoost) / `reg_lambda` (XGBoost) → regularization\n",
        "  * `scale_pos_weight` → handle class imbalance\n",
        "\n",
        "* Apply **early stopping** on a validation set to prevent overfitting.\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 4: Evaluation Metrics**\n",
        "\n",
        "Since the dataset is **imbalanced**, accuracy alone is misleading. Use:\n",
        "\n",
        "1. **Precision & Recall** – Focus on correctly identifying **defaulters**.\n",
        "2. **F1-Score** – Balanced metric combining precision and recall.\n",
        "3. **ROC-AUC** – Measures model’s ability to rank defaulters higher than non-defaulters.\n",
        "4. **Confusion Matrix** – Visualize True Positives, False Negatives (critical in finance).\n",
        "\n",
        "---\n",
        "\n",
        "## **Step 5: Business Benefits of the Model**\n",
        "\n",
        "1. **Reduce Loan Defaults**\n",
        "\n",
        "   * Identify high-risk customers before approving loans.\n",
        "\n",
        "2. **Better Risk-Based Pricing**\n",
        "\n",
        "   * Adjust interest rates according to predicted default risk.\n",
        "\n",
        "3. **Efficient Resource Allocation**\n",
        "\n",
        "   * Focus collections and monitoring on high-risk customers.\n",
        "\n",
        "4. **Regulatory Compliance & Transparency**\n",
        "\n",
        "   * Feature importance from boosting models provides insights for auditors and regulators.\n",
        "\n",
        "5. **Revenue Protection & Profit Maximization**\n",
        "\n",
        "   * By accurately predicting defaults, the company reduces losses and maximizes safe lending opportunities.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0pE-3P3UuUtF"
      }
    }
  ]
}